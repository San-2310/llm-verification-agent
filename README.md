# Verification Agent – Multi-Agent Investment Analyst (LLM Capstone)

## Overview

This module implements the **Verification Agent** for the Multi-Agent Investment Analyst capstone system.

The Verification Agent's goal is to cross-check the financial theses generated by other agents (Analyst & Thesis) and ensure that each investment claim is **consistent, data-backed, and logically sound**.

It combines:

1. **FinBERT** – for financial sentiment understanding of the thesis text.
2. **Groq-hosted LLM** (LLaMA 3.1 / Mixtral) – for deeper reasoning and validation between the analyst's quantitative data and the thesis's qualitative claims.

The output is a structured JSON verdict containing validity, reasoning, and confidence score.

---

## System Architecture (Simplified)

```

Analyst Agent → produces metrics (P/E, growth, EPS)
↓
Thesis Agent → produces human-like investment thesis
↓
Verification Agent (this module) → checks consistency and explains verdict

```

### Verification Flow:

1. **Input**: JSON containing analyst data and thesis text.
2. **FinBERT sentiment analysis** → understand tone (bullish, bearish, neutral).
3. **Groq LLM verification** → reasoning-based consistency check between data and thesis.
4. **Output**: JSON verdict `{ valid, reason, confidence }` + FinBERT sentiment.

---

## Models Used

### 1. FinBERT (from Hugging Face: `ProsusAI/finbert`)

- **Type**: Transformer-based BERT model fine-tuned on financial text.
- **Purpose**: Detects sentiment (positive, neutral, negative) in financial language.
- **Use in this project**:
  - Helps understand if the thesis is overly bullish/bearish.
  - Adds interpretability layer to reasoning output.

**Example:**

```python
get_finbert_sentiment("Apple has strong growth potential and is undervalued.")
# → {"sentiment": "positive", "confidence": 0.92}
```

---

### 2. Groq-hosted LLM (default: `LLaMA-3.1-8B-Instant`)

- **Provider**: Groq Cloud
- **API**: OpenAI-compatible (`https://api.groq.com/openai/v1/chat/completions`)
- **Purpose**: Validates logical and factual consistency between thesis claims and analyst data.
- **Strengths**:
  - Strong reasoning capabilities.
  - Zero cost (free API tier).
  - JSON-structured output enforced by system prompt.

#### Other supported Groq models:

| Model Name             | Description                 | Use Case                 |
| ---------------------- | --------------------------- | ------------------------ |
| `llama-3.1-8b-instant` | Lightweight, fast           | Ideal for demo |
| `llama3-70b-8192`      | High accuracy               | Heavy-duty validation    |
| `mixtral-8x7b-32768`   | Balanced mixture-of-experts | Deep reasoning           |
| `gemma-7b-it`          | Efficient instruction model | Backup option            |

---

## File Structure

```
verification_agent/
├── .env                      # contains GROQ_API_KEY
├── finbert_helper.py         # sentiment analysis with FinBERT
├── groq_client.py            # Groq LLM call (financial consistency check)
├── verification_agent.py     # main orchestrator
├── sample_input.json         # demo input
├── requirements.txt
└── README.md
```

---

## How It Works (Conceptually)

### 1. FinBERT Layer

- Extracts sentiment from the thesis.
- Gives insight into the tone (optimistic, neutral, pessimistic).

### 2. LLM Reasoning Layer (Groq)

- Takes the analyst's metrics (e.g., PE ratio, revenue growth).
- Reads the thesis claim.
- Checks for inconsistencies:
  - "Claim says 50% growth" vs "Data shows only 5% growth."
  - "Thesis says undervalued" vs "PE = 32 (usually overvalued)."
- Returns a JSON verdict explaining validity.

---

## Technical Flow Example

### Input (`sample_input.json`)

```json
{
  "company": "AAPL",
  "analyst_data": {
    "pe_ratio": 32,
    "revenue_growth": 0.05,
    "eps": 6.2,
    "notes": "Last 4 quarters revenue growth averaged 5%."
  },
  "thesis": {
    "stance": "bullish",
    "claim": "Apple has 50% revenue growth potential and is undervalued."
  }
}
```

### Output

```json
{
  "company": "AAPL",
  "thesis": "Apple has 50% revenue growth potential and is undervalued.",
  "finbert_sentiment": {
    "sentiment": "positive",
    "confidence": 0.89
  },
  "verification": {
    "valid": false,
    "reason": "Thesis claims 50% growth while data shows 5%. PE=32 indicates overvaluation.",
    "confidence": 0.22
  },
  "raw_llm_output": "{...raw model text...}"
}
```

---

## File-by-File Breakdown

### `finbert_helper.py`

- Loads FinBERT from Hugging Face.
- Provides `get_finbert_sentiment(text)` → returns label + confidence.

### `groq_client.py`

- Handles API call to Groq's OpenAI-compatible endpoint.
- Sends system + user messages enforcing JSON-only reply.
- Accepts model name parameter (`llama-3.1-8b-instant` by default).
- Returns model output text (raw JSON string).

### `verification_agent.py`

- Entry point.
- Combines FinBERT + Groq results.
- Parses and validates JSON from LLM output.
- Prints final verdict in structured form.

---

## Run the Agent

```bash
python verification_agent.py sample_input.json
```

**Expected output (truncated):**

```
Loading FinBERT model (this may download weights)...
{
  "company": "AAPL",
  "thesis": "...",
  "finbert_sentiment": {...},
  "verification": {...},
  "raw_llm_output": "{...}"
}
```

---

## Conceptual Takeaway

| Layer                  | Role                                       | Model                | Function                                    |
| ---------------------- | ------------------------------------------ | -------------------- | ------------------------------------------- |
| Sentiment Analysis     | Surface-level understanding of thesis tone | FinBERT              | Detects bias in text                        |
| Reasoning & Validation | Deep logical verification                  | LLaMA-3.1 (via Groq) | Compares analyst metrics with thesis claims |
| Output Aggregation     | Transparency & explainability              | Python wrapper       | Combines structured results                 |

This design mirrors how a human investment team works:

```
Analyst → makes calculations →
Thesis writer → builds narrative →
Verification agent → fact-checks + flags inconsistencies.
```

---

## Future Enhancements

- Add rule-based quick checks (e.g., "PE > 25 ⇒ not undervalued").
- Add multi-agent feedback loop → let verifier ask analyst for clarifications.
- Integrate FinBERT sentiment directly into the LLM prompt for richer reasoning.
- Enable async parallel requests for batch validation.

---

## Notes

- First FinBERT load will download ~440 MB model weights once.
- Groq API is OpenAI-compatible, so any OpenAI client or `requests` works.
- Keep `.env` out of version control (`.gitignore` included).
- Free Groq tier may throttle long prompts (>2000 tokens). Reduce `max_tokens` if needed.
